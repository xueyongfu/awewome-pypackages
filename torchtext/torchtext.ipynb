{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    * API一览  torchtext.data *\n",
    "    torchtext.data.Example : 用来表示一个样本，数据+标签\n",
    "    torchtext.vocab.Vocab: 词汇表相关\n",
    "    torchtext.data.Datasets: 数据集类，__getitem__ 返回 Example实例\n",
    "    torchtext.data.Field : 用来定义字段的处理方法（文本字段，标签字段）\n",
    "    创建 Example时的 预处理\n",
    "    batch 时的一些处理操作。\n",
    "    torchtext.data.Iterator: 迭代器，用来生成 batch\n",
    "    torchtext.datasets: 包含了常见的数据集."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Field对象"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "* tokenize传入一个函数，表示如何将文本str变成token\n",
    "* sequential表示是否切分数据，如果数据已经是序列化的了而且是数字类型的，则应该传递参数use_vocab = False和sequential = False\n",
    "* Field类还允许用户指定特殊标记（用于标记词典外词语的unk_token，用于填充的pad_token，用于句子结尾的eos_token以及用于句子开头的可选的init_token）。设置将第一维是batch还是sequence（第一维默认是sequential），并选择是否允许在运行时决定序列长度还是预先就决定好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from torchtext.data import Field\n",
    "\n",
    "tokenize = lambda x: x.split()\n",
    "\n",
    "TEXT = Field(sequential=True, tokenize=tokenize, lower=True)\n",
    "LABEL = Field(sequential=False, use_vocab=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 构建Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "> Fields知道怎么处理原始数据，现在我们需要告诉Fields去处理哪些数据。这就是我们需要用到Dataset的地方。Torchtext中有各种内置Dataset，用于处理常见的数据格式。 对于csv/tsv文件，TabularDataset类很方便。 以下是我们如何使用TabularDataset从csv文件读取数据的示例\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import TabularDataset\n",
    " \n",
    "# 多标签分类\n",
    "tv_datafields = [(\"id\", None), # 我们不会需要id，所以我们传入的filed是None\n",
    "                 (\"comment_text\", TEXT), (\"toxic\", LABEL),\n",
    "                 (\"severe_toxic\", LABEL), (\"threat\", LABEL),\n",
    "                 (\"obscene\", LABEL), (\"insult\", LABEL),\n",
    "                 (\"identity_hate\", LABEL)]\n",
    "\n",
    "# 读取训练集,验证集\n",
    "trn, vld = TabularDataset.splits(\n",
    "               path=\"data\", # 数据存放的根目录\n",
    "               train='train.csv', validation=\"valid.csv\",\n",
    "               format='csv',\n",
    "               skip_header=True, # 如果你的csv有表头, 确保这个表头不会作为数据处理\n",
    "               fields=tv_datafields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取测试集\n",
    "tst_datafields = [(\"id\", None), # 我们不会需要id，所以我们传入的filed是None\n",
    "                  (\"comment_text\", TEXT)]\n",
    "\n",
    "# 单独读取一个文件\n",
    "tst = TabularDataset(\n",
    "           path=\"data/test.csv\", # 文件路径\n",
    "           format='csv',\n",
    "           skip_header=True, # 如果你的csv有表头, 确保这个表头不会作为数据处理\n",
    "           fields=tst_datafields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一次性读取三个文件\n",
    "\n",
    "# 读取训练集,验证集,测试集\n",
    "train, val, test = TabularDataset.splits(\n",
    "        path='data', train='train.csv',\n",
    "        validation='valid.csv', test='test.csv', format='csv',\n",
    "        fields=[('Text', TEXT), ('Label', LABEL)])"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 词表"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "> Torchtext将单词映射为整数，但必须告诉它应该处理的全部单词。 \n",
    "\n",
    "> 在我们的例子中，我们可能只想在训练集上建立词汇表，所以我们运行代码：TEXT.build_vocab(trn)。这使得torchtext遍历训练集中的所有元素，检查TEXT字段的内容，并将其添加到其词汇表中。\n",
    "\n",
    "> Torchtext有自己的Vocab类来处理词汇。Vocab类在stoi属性中包含从word到id的映射，并在其itos属性中包含反向映射。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train)\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 构建迭代器"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "> Iterators具有一些NLP特有的便捷功能。\n",
    "\n",
    "* 对于验证集和训练集合使用BucketIterator.splits(),目的是自动进行shuffle和padding，并且为了训练效率期间，尽量把句子长度相似的shuffle在一起。\n",
    "* 对于测试集用Iterator，因为不用sort。\n",
    "* sort 是对全体数据按照升序顺序进行排序，而sort_within_batch仅仅对一个batch内部的数据进行排序。\n",
    "* sort_within_batch参数设置为True时，按照sort_key按降序对每个小批次内的数据进行降序排序。当你想对padded序列使用pack_padded_sequence转换为PackedSequence对象时，这是必需的。\n",
    "* 注意sort和shuffle默认只是对train=True字段进行的，但是train字段默认是True。所以测试集合可以这么写testIter = Iterator(tst, batch_size = 64, device =-1, train=False)写法等价于下面的一长串写法。\n",
    "* repeat 是否连续的训练无数个batch ,默认是False\n",
    "* device 可以是torch.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Iterator, BucketIterator\n",
    " \n",
    "train_iter, val_iter = BucketIterator.splits((trn, vld), \n",
    "                                             # 我们把Iterator希望抽取的Dataset传递进去\n",
    "                                             batch_sizes=(25, 25),\n",
    "                                             device='-1', # 如果要用GPU，这里指定GPU的编号\n",
    "                                             sort_key=lambda x: len(x.comment_text), # BucketIterator 依据什么对数据分组\n",
    "                                             sort_within_batch=False,\n",
    "                                             repeat=False   # repeat设置为False，因为我们想要包装这个迭代器层。\n",
    "                                             )\n",
    "         \n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<torchtext.data.iterator.BucketIterator at 0x7fcb34d60278>"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "train_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_iter = Iterator(tst, batch_size=64, \n",
    "                     device='-1', \n",
    "                     sort=False, \n",
    "                     sort_within_batch=False, \n",
    "                     repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python36964bitpy36torch12conda69cefb4d206245c4959c1e16372a73c3",
   "language": "python",
   "display_name": "Python 3.6.9 64-bit ('py36_torch1.2': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}