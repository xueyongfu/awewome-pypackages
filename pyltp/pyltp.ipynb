{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python36964bitpython36conda887ba476e9aa484bad06fe49784f25a0",
   "display_name": "Python 3.6.9 64-bit ('python36': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "from pyltp import Segmentor, Postagger, Parser ,SementicRoleLabeller,NamedEntityRecognizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nlpLtp:\n",
    "    \n",
    "    def __init__(self):\n",
    "        MODELDIR = '/home/xyf/models/chinese/ltp_model'\n",
    "\n",
    "        #系统切词\n",
    "        self.segmentor = Segmentor()\n",
    "        self.segmentor.load(os.path.join(MODELDIR, \"cws.model\"))\n",
    "\n",
    "        self.postagger = Postagger()\n",
    "        self.postagger.load(os.path.join(MODELDIR, \"pos.model\"))\n",
    "\n",
    "        self.namedentityrecognizer = NamedEntityRecognizer()\n",
    "        self.namedentityrecognizer.load(os.path.join(MODELDIR, \"ner.model\"))\n",
    "\n",
    "        self.parser = Parser()\n",
    "        self.parser.load(os.path.join(MODELDIR, \"parser.model\"))\n",
    "\n",
    "        self.labeller = SementicRoleLabeller()\n",
    "        self.labeller.load(os.path.join(MODELDIR, \"pisrl.model\"))\n",
    "\n",
    "        self.parse_dict =  {\"SBV\":\"主谓关系\", \"VOB\":\"动宾关系\", \"IOB\":\"间宾关系\", \"FOB\":\"前置宾语\", \"DBL\":\"兼语\",\n",
    "                           \"ATT\":\"定中关系\", \"ADV\":\"状中关系\", \"CMP\":\"动补关系\", \"POB\":\"介宾关系\", \"LAD\":\"左附加关系\",\n",
    "                           \"RAD\":\"右附加关系\", \"IS\":\"独立结构\",\"COO\":\"并列关系\", \"HED\":\"核心关系\", \"WP\":\"标点\"}\n",
    "\n",
    "    def sent_segment(self, sentence):\n",
    "        words_ltp = self.segmentor.segment(sentence)\n",
    "        words_list = [w for w in words_ltp]\n",
    "        return words_list\n",
    "\n",
    "    def sent_pos(self, sentence):\n",
    "        words = self.segmentor.segment(sentence)\n",
    "        postags = self.postagger.postag(words)\n",
    "        return postags\n",
    "\n",
    "    def sent_ner(self, sentence):\n",
    "        words = self.segmentor.segment(sentence)\n",
    "        postags = self.postagger.postag(words)\n",
    "        netags = self.namedentityrecognizer.recognize(words, postags)\n",
    "        return netags\n",
    "\n",
    "    def sent_syntax(self, sentence):\n",
    "        words = self.segmentor.segment(sentence)\n",
    "        postags = self.postagger.postag(words)\n",
    "        parsing = self.parser.parse(words, postags)\n",
    "        syntax = \"  \".join(\"%d:%s\" % (pars.head, pars.relation) for pars in parsing)\n",
    "        return parsing, syntax\n",
    "\n",
    "    def sent_syntax_self(self,sentence):\n",
    "        print ('原文本：' + sentence)\n",
    "        words = self.sent_segment(sentence)\n",
    "        print('分词结果：' + str(words))\n",
    "        postags = self.sent_pos(sentence)\n",
    "        print('词性标注结果：' + str([a for a in postags]))\n",
    "        parsing = self.parser.parse(words, postags)\n",
    "        parsing_a = \"  \".join(\"%d:%s\" % (pars.head, pars.relation) for pars in parsing)\n",
    "        print('句法分析结果：' + parsing_a)\n",
    "\n",
    "        parsing_b = zip(words, parsing)\n",
    "        for par in parsing_b:\n",
    "            if par[1].relation in ['WP', 'HED']:\n",
    "                print('\"'+par[0]+'\"是'+self.parse_dict[par[1].relation], end=',  ')\n",
    "            else:\n",
    "                print('\"'+par[0]+'\"'+'与'+'\"'+words[par[1].head-1]+'\"'+'：'+self.parse_dict[par[1].relation], end=',  ')\n",
    "\n",
    "    def sent_role(self, sentence):\n",
    "        words = self.sent_segment(sentence)\n",
    "        postags = self.sent_pos(sentence)\n",
    "        parsing = self.parser.parse(words, postags)\n",
    "\n",
    "        roles = self.labeller.label(words, postags, parsing)\n",
    "        for role in roles:  #roles是谓词\n",
    "            print(role.index, \"\".join(\n",
    "                [\"%s:(%d,%d)\" % (arg.name, arg.range.start, arg.range.end) for arg in role.arguments]))\n",
    "            \n",
    "            for arg in role.arguments:\n",
    "                if arg.name == 'A1':\n",
    "                    words_list=words[arg.range.start:arg.range.end+1]\n",
    "                    print(''.join(words_list))\n",
    "\n",
    "# 构造一个对象\n",
    "nlpltp = nlpLtp()\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['客户', '来电', '反映', '之前区', '营业厅', '办理', '卡', '的', '时候', '工作', '人员', '有', '为', '客户', '参与', 'AA421066', '_', '全国', '不', '限量', '68', '包', '打98', '（', '6', '个', '月', '）', '和', '办理', 'ACBZ14195', '新', '爱', '家', '88', '（', 'V2.0', '）', '，', '13408470690', '全球通', '营销', '执行', '（', '电子', '渠道', '营销', '）', '2039003395367', '2019-06-28', '12', ':', '09', ':', '45', 'ane130007', '(', '成都', '温江', '分公司', '龙翔', '通讯', '永兴路', '延迟', '结酬', '至', '202001', '李川川', ')', '营业厅', '无', '电子化', '渠道', '营销', '执行', '0.00', '身份', '证件']\n"
    }
   ],
   "source": [
    "print(nlpltp.sent_segment('客户来电反映之前区营业厅办理卡的时候工作人员有为客户参与AA421066_全国不限量68包打98（6个月）和办理ACBZ14195 新爱家88（V2.0），13408470690 全球通 营销执行（电子渠道营销） 2039003395367  2019-06-28 12:09:45 ane130007(成都温江分公司龙翔通讯永兴路延迟结酬至202001李川川)  营业厅 无 电子化渠道营销执行 0.00 身份证件 '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "['nh', 'n', 'nt', 'v', 'n', 'u', 'wp', 'r', 'v', 'd', 'a']\n"
    }
   ],
   "source": [
    "print(list(nlpltp.sent_pos('李克强总理今天来我家了,我感到非常荣幸')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['O', 'O', 'O', 'O', 'O', 'O', 'S-Ns', 'O', 'B-Ns', 'E-Ns', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
    }
   ],
   "source": [
    "print(list(nlpltp.sent_ner('【手机】市民反映：浦东新区曹路镇顾曹公路市场路有乱设摊，市民未提供具体地址，具体设摊时间，诉求：请管理部门核实后尽快协调制止。')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'{\"segment\": [\"\\\\u674e\\\\u514b\\\\u5f3a\", \"\\\\u603b\\\\u7406\", \"\\\\u4eca\\\\u5929\", \"\\\\u6765\", \"\\\\u6211\\\\u5bb6\", \"\\\\u4e86\", \",\", \"\\\\u6211\", \"\\\\u611f\\\\u5230\", \"\\\\u975e\\\\u5e38\", \"\\\\u8363\\\\u5e78\"], \"pos\": [\"nh\", \"n\", \"nt\", \"v\", \"n\", \"u\", \"wp\", \"r\", \"v\", \"d\", \"a\"], \"ner\": [\"S-Nh\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\"]}'"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "a={\n",
    "    \"segment\":[\"李克强\", \"总理\", \"今天\", \"来\", \"我家\", \"了\", \",\", \"我\", \"感到\", \"非常\", \"荣幸\"],\n",
    "    \"pos\":[\"nh\", \"n\", \"nt\", \"v\", \"n\", \"u\", \"wp\", \"r\", \"v\", \"d\", \"a\"],\n",
    "    \"ner\":[\"S-Nh\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\"]\n",
    "}\n",
    "json.dumps(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "(<pyltp.VectorOfParseResult object at 0x7fdd930f38d0>, '2:ATT  4:SBV  4:ADV  0:HED  4:VOB  4:RAD  4:WP  9:SBV  4:COO  11:ADV  9:VOB')\n"
    }
   ],
   "source": [
    "print(nlpltp.sent_syntax('李克强总理今天来我家了,我感到非常荣幸'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "原文本：近日，一条男子高铁吃泡面被女乘客怒怼的视频引发热议\n分词结果：['近日', '，', '一', '条', '男子', '高铁', '吃', '泡面', '被', '女', '乘客', '怒怼', '的', '视频', '引发', '热议']\n词性标注结果：['nt', 'wp', 'm', 'q', 'n', 'n', 'v', 'n', 'p', 'b', 'n', 'v', 'u', 'n', 'v', 'n']\n句法分析结果：15:ADV  1:WP  4:ATT  6:ATT  6:ATT  7:SBV  14:ATT  7:VOB  12:ADV  11:ATT  9:POB  14:ATT  12:RAD  15:SBV  0:HED  15:VOB\n\"近日\"与\"引发\"：状中关系,  \"，\"是标点,  \"一\"与\"条\"：定中关系,  \"条\"与\"高铁\"：定中关系,  \"男子\"与\"高铁\"：定中关系,  \"高铁\"与\"吃\"：主谓关系,  \"吃\"与\"视频\"：定中关系,  \"泡面\"与\"吃\"：动宾关系,  \"被\"与\"怒怼\"：状中关系,  \"女\"与\"乘客\"：定中关系,  \"乘客\"与\"被\"：介宾关系,  \"怒怼\"与\"视频\"：定中关系,  \"的\"与\"怒怼\"：右附加关系,  \"视频\"与\"引发\"：主谓关系,  \"引发\"是核心关系,  \"热议\"与\"引发\"：动宾关系,  None\n"
    }
   ],
   "source": [
    "print(nlpltp.sent_syntax_self('近日，一条男子高铁吃泡面被女乘客怒怼的视频引发热议'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "3 TMP:(0,1)A0:(2,2)A1:(4,7)\n看恐龙来了\n4 A1:(5,5)\n恐龙\nNone\n"
    }
   ],
   "source": [
    "# 语义角色分析\n",
    "# 核心的语义角色为 A0-5 六种，A0 通常表示动作的施事，A1通常表示动作的影响等，A2-5 根据谓语动词不同会有不同的语义含义。其余的15个语义角色为附加语义角色，如LOC 表示地点，TMP 表示时间等。附加语义角色列表见LTP官方说明文档\n",
    "\n",
    "print(nlpltp.sent_role('今天上午我想看恐龙来了'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}